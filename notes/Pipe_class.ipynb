{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import pickle\n",
    "import string\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "lst_stopwords = nltk.corpus.stopwords.words('russian')\n",
    "lst_stopwords.extend(['…', '«', '»', '...'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer, stopwords):\n",
    "\n",
    "    text = str(text).lower()  \n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  \n",
    "    text = re.sub(r\"\\s+\", \" \", text)  \n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  \n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  \n",
    "    text = re.sub(r\"[0-9]\", \"\", text)\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  \n",
    "\n",
    "    tokens = tokenizer(text)  \n",
    "    tokens = [t for t in tokens if not t in lst_stopwords]  \n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  \n",
    "    tokens = [t for t in tokens if len(t) > 1] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_tokens(df_raw):\n",
    "    text_columns = [\"Комментарий\"]\n",
    "    # df_raw['content'] = df_raw['content'].fillna(\" \")\n",
    "    # for col in text_columns:\n",
    "    #     df_raw[col] = df_raw[col].astype(str)\n",
    "    # создаем текст основанный на content title и tag\n",
    "    df_raw[\"text\"] = df_raw[text_columns].apply(lambda x: \" | \".join(x), axis=1)\n",
    "    df_raw[\"tokens\"] = df_raw[\"text\"].map(lambda x: clean_text(x, word_tokenize, lst_stopwords))\n",
    "    _, idx = np.unique(df_raw[\"tokens\"], return_index=True)\n",
    "    df_raw = df_raw.iloc[idx, :]\n",
    "\n",
    "    # Remove empty values\n",
    "    df_raw = df_raw.loc[df_raw.tokens.map(lambda x: len(x) > 0), [\"text\", \"tokens\"]]\n",
    "    return df_raw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl=StandardScaler()\n",
    "w2v_model = gensim.models.Word2Vec.load(\"word2vec.model\")\n",
    "with open('model_rf.pkl', 'rb') as fid:\n",
    "    sk_model= pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "awesome_text=['курьер опоздал']\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "def get_lable(some_text):\n",
    "    doc = nlp(some_text[0])\n",
    "    time_slov={}\n",
    "    for ent in doc.ents:\n",
    "        time_slov[f'{ent.label_}']=ent.text\n",
    "    exper=pd.DataFrame(some_text,columns=['Комментарий'])\n",
    "    vectorized_dox=(vectorize(prep_tokens(exper).iloc[:,1], model=w2v_model))\n",
    "    return vectorized_dox,time_slov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_model.predict(get_lable(awesome_text)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
